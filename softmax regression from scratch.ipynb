{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzJyfHY10bytEZpbRx1470",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvish7/deep_learning_handson/blob/master/softmax%20regression%20from%20scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gymzRFC8_97P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install mxnet==1.6.0b20190926\n",
        "!pip install d2l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atr_c5ef9IMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import d2l\n",
        "import torch, torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.distributions import normal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwEsUvVlEE-c",
        "colab_type": "text"
      },
      "source": [
        "creating data loader instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62Nx0-9_9iCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 256\n",
        "trans = transforms.ToTensor()\n",
        "\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iGXNDXFEAFg",
        "colab_type": "text"
      },
      "source": [
        "Random init of weights and bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCjLyw5BDqbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "\n",
        "W = normal.Normal(loc = 0, scale = 0.01).sample((num_inputs, num_outputs))\n",
        "b = torch.zeros(num_outputs)\n",
        "\n",
        "W.requires_grad_(True)\n",
        "b.requires_grad_(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuP11g9FENmI",
        "colab_type": "text"
      },
      "source": [
        "implementing softmax "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y42tsMqD991",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    X_exp = torch.exp(X)\n",
        "    norm_const = torch.sum(X_exp, dim=1, keepdim=True)\n",
        "    return X_exp / norm_const"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paQoCuABEbnw",
        "colab_type": "text"
      },
      "source": [
        "checking of softmax fun works or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3ZNRwTNEVRr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "50b02645-6169-40cf-91ab-be2fed9d6932"
      },
      "source": [
        "X = normal.Normal(loc = 0, scale = 1).sample((2, 5))\n",
        "X_prob = softmax(X)\n",
        "X_prob, torch.sum(X_prob, dim=1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0440, 0.0478, 0.3440, 0.3804, 0.1839],\n",
              "         [0.4796, 0.0274, 0.0385, 0.2239, 0.2307]]), tensor([1.0000, 1.0000]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjT2rKEKErYT",
        "colab_type": "text"
      },
      "source": [
        "model defination"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ESs_75lErA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def net(X):\n",
        "    x = torch.matmul(X.reshape((-1, num_inputs)), W) + b\n",
        "    return softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOuL8SPHE8-K",
        "colab_type": "text"
      },
      "source": [
        "defining cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAjdE6tmEhhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "    return -torch.gather(y_hat, 1, y.unsqueeze(dim=1)).log() #hats off to this, i was thinking of using loop :-p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbD5UrisFUVu",
        "colab_type": "text"
      },
      "source": [
        "fun for evaluating accuracy on test data -- taken from d2l"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTJaLGHKFLIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_accuracy(data_iter, net):\n",
        "    acc_sum, n = 0.0, 0\n",
        "    for X, y in data_iter:\n",
        "        acc_sum += (net(X).argmax(dim=1) == y).sum().item()\n",
        "        n += y.size()[0]  # y.size()[0] = batch_size\n",
        "    return acc_sum / n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TKo4MUiFkgq",
        "colab_type": "text"
      },
      "source": [
        "training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MRZWNdBFkRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs, lr = 5, 0.1\n",
        "\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params=None, lr=None, trainer=None):\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
        "        for X, y in train_iter:\n",
        "            y_hat = net(X)\n",
        "            l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            if trainer is None:\n",
        "                d2l.sgd(params, lr, batch_size)\n",
        "            else:\n",
        "                # This will be illustrated in the next section\n",
        "                trainer.step(batch_size)\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
        "            n += y.size()[0]\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8Gy_f34FiU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjlJ5qzRFanu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}