{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0weNcJAKql6",
        "colab_type": "text"
      },
      "source": [
        "# Creating a Custom block, block is just a collection of layers\n",
        "\n",
        "Necessary steps for defining a block\n",
        "\n",
        "1. Provision of accepting the input\n",
        "2. Provision to produce a meaningful output. This is typically encoded in what we will call the forward function. It allows us to invoke a block via net(X) to obtain the desired output.\n",
        "3. It needs to produce a gradient with regard to its input when invoking backward. Autograd takes care of this part.\n",
        "4. It needs to store parameters that are inherent to the block.\n",
        "5. Obviously it also needs to initialize these parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUNgwvEeKyJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "  # Call the constructor of the parent class Module to perform the\n",
        "  # necessary initialization.\n",
        "    super(CustomBlock, self).__init__(**kwargs)\n",
        "    self.hidden = nn.Sequential(nn.Linear(20,256),nn.ReLU()) # Hidden layer\n",
        "    self.output = nn.Linear(256,10) # Output layer\n",
        "\n",
        "  # Define the forward computation of the model, that is, how to return the \n",
        "  # required model output based on the input x\n",
        "  def forward(self, x):\n",
        "    return self.output(self.hidden(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbqA62GyLUBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "94aa520a-b3bd-4c63-8f74-26c7194d2748"
      },
      "source": [
        "net1 = CustomBlock()\n",
        "x = torch.randn(2,20)\n",
        "net1(x)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2600, -0.2599,  0.1142,  0.0969,  0.0293, -0.2515,  0.0657,  0.0092,\n",
              "          0.2506,  0.1843],\n",
              "        [ 0.2142, -0.2933,  0.2687, -0.0835,  0.2717, -0.0418, -0.1790,  0.4063,\n",
              "          0.1690, -0.1888]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HZhTVoyIw1i",
        "colab_type": "text"
      },
      "source": [
        "#Custom implementation of Sequential layer (i.e. nn.sequential)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URxP3NOyJr-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K9g8U-nItRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MySequential(nn.Sequential):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(MySequential, self).__init__(**kwargs)\n",
        "  def add_module(self, block):\n",
        "    # Here we assume block has a unique name. We save it in the member \n",
        "    # variable _children of the Block class.\n",
        "    self._modules[block] = block\n",
        "\n",
        "  def forward(self, x):\n",
        "    # OrderedDict guarantees that members will be traversed in the order\n",
        "    # they were added\n",
        "    for block in self._modules.values():\n",
        "      x = block(x)\n",
        "    return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn0IuQGNJqDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7c5a9c4e-eead-4fdd-8c5b-1d860ecca562"
      },
      "source": [
        "net = MySequential()\n",
        "net.add_module(nn.Linear(20,256))\n",
        "net.add_module(nn.ReLU())\n",
        "net.add_module(nn.Linear(256,10))\n",
        "x = torch.randn(2,20)\n",
        "net(x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1256,  0.1116,  0.2782, -0.0731, -0.1832,  0.0491,  0.1077, -0.1340,\n",
              "          0.0596, -0.0135],\n",
              "        [ 0.2973,  0.1412, -0.0242,  0.0690,  0.0182, -0.0184, -0.2834, -0.0585,\n",
              "         -0.0611, -0.2731]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4-CXRVEJywy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}